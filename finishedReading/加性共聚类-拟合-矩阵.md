### 主要问题：

- Matrix completion and approximation are popular tools to capture a user's preferences for recommendation and to approximate missing data. Instead of using low-rank factorization we take a drastically different approach, based on the simple insight that an additive model of co-clusterings allows one to approximate matrices efficiently. This allows us to build a concise model that, per bit of model learned, significantly beats all factorization approaches in matrix completion. Even more surprisingly, we find that summing over small co-clusterings is more effective in modeling matrices than classic co-clustering, which uses just one large partitioning of the matrix. 
- 加性共聚类-拟合-矩阵效果优越于矩阵的低秩分解
- Following Occam’s razor principle, the fact that our model is more concise and yet just as accurate as more complex models suggests that it better captures the latent preferences and decision making processes present in the real world. 
- 奥卡姆剃刀要求模型更简洁
- Using a linear combination of co-clusterings corresponds to a rather different interpretation of user preferences and movie properties. Matrix factorization assumes that a movie preference is based on a weighted sum of preferences for different genres, with the movie properties being represented in vectorial form. Therefore, even if a movie is a comedy and the user likes comedies, the model still also offsets by how scary the movie is and does the user like scary movies.
- 矩阵分解假设电影偏好是基于不同类型的偏好的加权和，以矢量形式表示电影属性。因此，即使电影是喜剧，用户喜欢喜剧，该模型还抵消了恐怖电影是多么可怕，用户喜欢恐怖电影。


### 解决方法：
- Recent state-of-the-art methods have relied on learning even larger, more complex factorization models, often taking nontrivial combinations of multiple submodels. Such complex models are increasingly difficult to interpret, use large amounts of memory, and are often difficult integrate into larger systems.
- Co-cluster 线性组合
- Co-clustering finds a clustering of the rows and columns of a matrix R so as to partition R into blocks that are highly similar. 
- By taking a linear combination of co-clusterings we benefit from both perspectives: by modeling the discrete nature of attributes we can avoid the cost of high-dimensional factorization models, and by adding the preferences for different attributes we can avoid the large models necessary to cover all combinations of attributes. 
- Therefore, by taking linear combinations of co-clusterings we can efficiently take all of these attributes into account
- Matrix Approximation ：用indicator matrix I 表示是否有值。
![1](https://github.com/csmaterial/paper/blob/master/finishedReading/picture/%E6%8D%951%E8%8E%B7.PNG)
- stencil:![1](https://github.com/csmaterial/paper/blob/master/finishedReading/picture/%E6%8D%95%E8%8E%B7.PNG)![1](https://github.com/csmaterial/paper/blob/master/finishedReading/picture/%E6%8D%952%E8%8E%B7.PNG)

1. 聚类算法：输入：矩阵M（n * m)权重W（n * m)和簇的数目k，从M中随机抽取组成k个向量组合V = {v1,...,vk} (k * m);当not converged时，进行kmeans搜索，返回存储每一行所属哪一类的c，和簇中心V，以及计数L。![1](https://github.com/csmaterial/paper/blob/master/finishedReading/picture/cluster.PNG)
2. 矩阵拟合：评分矩阵R（n * m)权重W（n * m)和簇的数目kn、km，最大stencils数量为s，遍历s，使用上面的聚类算法（kn），输出结果放入聚类算法（km），对kn，km遍历，取均值放入T中，再将T放入S
3. Backfitting：在R上运行上述行和列聚类的结果是由第一行然后列聚类获得的聚类组成的单个模板S（T; c; d）。可能需要在行和列聚类之间交替进行进一步的修改。但是，如上所述，额外的模板只能进一步减少目标函数 - 保证收敛到局部最小值，同时解决方案质量与k均值聚类相同。因此，我们将剩余的^ R =R􀀀S（T; c; d）作为起点来学习一个新的模板。通过反复学习以前模板残差的新模板，

