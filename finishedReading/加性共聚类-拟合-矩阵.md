### 主要问题：

Matrix completion and approximation are popular tools to capture a user's preferences for recommendation and to approximate missing data. Instead of using low-rank factorization we take a drastically different approach, based on the simple insight that an additive model of co-clusterings allows one to approximate matrices efficiently. This allows us to build a concise model that, per bit of model learned, significantly beats all factorization approaches in matrix completion. Even more surprisingly, we find that summing over small co-clusterings is more effective in modeling matrices than classic co-clustering, which uses just one large partitioning of the matrix. 
加性共聚类-拟合-矩阵效果优越于矩阵的低秩分解
Following Occam’s razor principle, the fact that our model is more concise and yet just as accurate as more complex models suggests that it better captures the latent preferences and decision making processes present in the real world. 
奥卡姆剃刀要求模型更简洁
Using a linear combination of co-clusterings corresponds to a rather different interpretation of user preferences and movie properties. Matrix factorization assumes that a movie preference is based on a weighted sum of preferences for different genres, with the movie properties being represented in vectorial form. Therefore, even if a movie is a comedy and the user likes comedies, the model still also offsets by how scary the movie is and does the user like scary movies.
矩阵分解假设电影偏好是基于不同类型的偏好的加权和，以矢量形式表示电影属性。因此，即使电影是喜剧，用户喜欢喜剧，该模型还抵消了恐怖电影是多么可怕，用户喜欢恐怖电影。


### 解决方法：
Recent state-of-the-art methods have relied on learning even larger, more complex factorization models, often taking nontrivial combinations of multiple submodels. Such complex models are increasingly difficult to interpret, use large amounts of memory, and are often difficult integrate into larger systems.
1.Co-cluster 线性组合
Co-clustering finds a clustering of the rows and columns of a matrix R so as to partition R into blocks that are highly similar. 
By taking a linear combination of co-clusterings we benefit from both perspectives: by modeling the discrete nature of attributes we can avoid the cost of high-dimensional factorization models, and by adding the preferences for different attributes we can avoid the large models necessary to cover all combinations of attributes. 
Therefore, by taking linear combinations of co-clusterings we can efficiently take all of these attributes into account

